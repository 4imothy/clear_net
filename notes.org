#+startup: overview
* Optimization Algorithms
- [[https://en.wikipedia.org/wiki/Stochastic_gradient_descent][See Extensions and Variants (wikipedia)]]
- Adaptive Gradient Algorithm (AdaGrad)
- Root Mean Square Propagation (RMSProp)
- The algorithms that have a perparameter learning rate must have that rate
  saved to enable training on an already saved net, create a seperate
  cn_alloc_net_from_file function with this functionality
