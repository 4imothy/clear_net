#+startup: overview
* CNN Ideas
Filter consists of multiple kernels stacked on top of eachother.
** Sources
[[https://www.youtube.com/watch?v=Lakz2MoHy6o][Convolutional Neural Network from Scratch | Mathematics & Python Code]]
[[https://www.youtube.com/watch?v=z9hJzduHToc&t=2s][Backpropagation in Convolutional Neural Networks from Scratch | The Weights]]
[[https://towardsdatascience.com/the-most-intuitive-and-easiest-guide-for-convolutional-neural-network-3607be47480][The Most Intuitive and Easiest Guide for Convolutional Neural Network by Jiwon Jeong]]
** Terms
*Stride:* the amount of pixels between each movement of the filter over the image
*Valid Cross Correlation:* The kernel starts entirely on the input. The kernel is contained fully in the input.
Stop sliding when it hits the border
  - Size of the result is $l_{input} - l_{kernel} + 1$
*Full Cross Correlation:* The kernel starts applying as soon as there is an input element. Start in the top left corner with one element of the kernel on one element of the input. End in the bottom right with one element cross over
  - Size of the result is  $l_{input} + l_{kernel} - 1$
*Same Cross Correlation:* The input is padded and then /valid cross correlation/ is done on it, make the data at function return $0$ if out of bounds, if the kernel has odd length then pad the input by distance from center number to the output. In $3x3$ pad it by one. In $5x5$ pad it by $2$, output is the same size
** Concepts
- Use cross corelation instead of convolution, same results in practice
- In convolution, get the size of each layer and when they change do
  downsampling (maxpool, avgpool)
- Each pixel resulting from a filter goes through a bias and activation
*** To turn conv -> dense use global pooling
  - Global Max Pooling:
    - we take the maximum value and return that as an individual element
    in the vector representation of interest
  - Global Average Pooling:
    - compute the average of all pixels in each feature map and return
    that as a single element
  - Always go from conv layers to dense layers, doing convolutions on one dimensional data is meaningless
** Code Ideas
- Why do input and kernel have to be /2d/ can't they just be /1d/
*** Conv Layer Struct
- Number of filters is arbitrary but the number of channels is the number of filters of the previous layer = number of channels
  - Each filter has number of channel layers with one bias term
  - Multiply each kernel with its channel's values and then add the values of each channel and sum up all the values then add the bias, repeat that for each pixel
  - Repeat that for each filter
*** Pooling
- When changing the size the next image should be /1/n/ the size of the image before, where /n % 2 == 0/
** Things to Do Later
- Stride, dilation, transpose, padding to be changed in the implementation
* Optimization Algorithms
- [[https://en.wikipedia.org/wiki/Stochastic_gradient_descent][See Extensions and Variants (wikipedia)]]
- Adaptive Gradient Algorithm (AdaGrad)
- Root Mean Square Propagation (RMSProp)
- The algorithms that have a perparameter learning rate must have that rate
  saved to enable training on an already saved net, create a seperate
  cn_alloc_net_from_file function with this functionality
